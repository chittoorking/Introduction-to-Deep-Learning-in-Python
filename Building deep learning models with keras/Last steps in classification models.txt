You'll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called df.
You'll take information about the passengers and predict which ones survived.

The predictive variables are stored in a NumPy array predictors. The target to predict is in df.survived, though you'll have to manipulate it for Keras. 
The number of predictive features is stored in n_cols.

Here, you'll use the 'sgd' optimizer, which stands for Stochastic Gradient Descent. You'll learn more about this in the next chapter!

Instructions
Convert df.survived to a categorical variable using the to_categorical() function.
Specify a Sequential model called model.
Add a Dense layer with 32 nodes. Use 'relu' as the activation and (n_cols,) as the input_shape.
Add the Dense output layer. Because there are two outcomes, it should have 2 units, and because it is a classification model, the activation should be 'softmax'.
Compile the model, using 'sgd' as the optimizer, 'categorical_crossentropy' as the loss function,
and metrics=['accuracy'] to see the accuracy (what fraction of predictions were correct) at the end of each epoch.
Fit the model using the predictors and the target.

# Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical

# Convert the target to categorical: target
target = to_categorical(df.survived)

# Set up the model
model = Sequential()

# Add the first layer
model.add(Dense(32,activation='relu',input_shape=(n_cols,)))

# Add the output layer
model.add(Dense(2,activation='softmax'))

# Compile the model
model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])

# Fit the model
model.fit(predictors,target)


output:

Epoch 1/10

 32/891 [>.............................] - ETA: 28s - loss: 7.6250 - acc: 0.2188
704/891 [======================>.......] - ETA: 0s - loss: 2.4436 - acc: 0.6037 
891/891 [==============================] - 1s - loss: 2.5170 - acc: 0.5948     
Epoch 2/10

 32/891 [>.............................] - ETA: 0s - loss: 1.1922 - acc: 0.3125
736/891 [=======================>......] - ETA: 0s - loss: 1.2050 - acc: 0.6019
891/891 [==============================] - 0s - loss: 1.1834 - acc: 0.6083     
Epoch 3/10

 32/891 [>.............................] - ETA: 0s - loss: 2.1141 - acc: 0.5000
736/891 [=======================>......] - ETA: 0s - loss: 0.8235 - acc: 0.6522
891/891 [==============================] - 0s - loss: 0.7783 - acc: 0.6700     
Epoch 4/10

 32/891 [>.............................] - ETA: 0s - loss: 0.7271 - acc: 0.5625
736/891 [=======================>......] - ETA: 0s - loss: 0.6990 - acc: 0.6658
891/891 [==============================] - 0s - loss: 0.7257 - acc: 0.6689     
Epoch 5/10

 32/891 [>.............................] - ETA: 0s - loss: 0.6173 - acc: 0.5938
672/891 [=====================>........] - ETA: 0s - loss: 0.6329 - acc: 0.6667
891/891 [==============================] - 0s - loss: 0.6529 - acc: 0.6588     
Epoch 6/10

 32/891 [>.............................] - ETA: 0s - loss: 0.4729 - acc: 0.7500
608/891 [===================>..........] - ETA: 0s - loss: 0.5998 - acc: 0.6908
891/891 [==============================] - 0s - loss: 0.6164 - acc: 0.6936     
Epoch 7/10

 32/891 [>.............................] - ETA: 0s - loss: 0.6730 - acc: 0.5938
512/891 [================>.............] - ETA: 0s - loss: 0.6423 - acc: 0.6621
891/891 [==============================] - 0s - loss: 0.6302 - acc: 0.6880     
Epoch 8/10

 32/891 [>.............................] - ETA: 0s - loss: 0.5364 - acc: 0.7188
544/891 [=================>............] - ETA: 0s - loss: 0.6413 - acc: 0.6691
891/891 [==============================] - 0s - loss: 0.6214 - acc: 0.6846     
Epoch 9/10

 32/891 [>.............................] - ETA: 0s - loss: 0.6651 - acc: 0.6250
640/891 [====================>.........] - ETA: 0s - loss: 0.6042 - acc: 0.6953
891/891 [==============================] - 0s - loss: 0.5970 - acc: 0.7015     
Epoch 10/10

 32/891 [>.............................] - ETA: 0s - loss: 0.4803 - acc: 0.7500
576/891 [==================>...........] - ETA: 0s - loss: 0.6377 - acc: 0.6788
891/891 [==============================] - 0s - loss: 0.6409 - acc: 0.6790     
<keras.callbacks.History at 0x7f2005a3f908>

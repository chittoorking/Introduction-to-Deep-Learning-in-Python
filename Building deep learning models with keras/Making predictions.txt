The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. 
Use model to make predictions on your new data.

In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.

Instructions
Create your predictions using the model's .predict() method on pred_data.
Use NumPy indexing to find the column corresponding to predicted probabilities of survival being True. This is the second column (index 1) of predictions.
Store the result in predicted_prob_true and print it.

# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]

# print predicted_prob_true
print(predicted_prob_true)

output:

Epoch 1/10

 32/800 [>.............................] - ETA: 30s - loss: 5.3679 - acc: 0.3438
352/800 [============>.................] - ETA: 1s - loss: 2.9146 - acc: 0.5938 
736/800 [==========================>...] - ETA: 0s - loss: 2.8892 - acc: 0.5380
800/800 [==============================] - 1s - loss: 2.9048 - acc: 0.5375     
Epoch 2/10

 32/800 [>.............................] - ETA: 0s - loss: 2.4982 - acc: 0.5625
384/800 [=============>................] - ETA: 0s - loss: 2.2550 - acc: 0.6302
736/800 [==========================>...] - ETA: 0s - loss: 2.9141 - acc: 0.5910
800/800 [==============================] - 0s - loss: 2.8557 - acc: 0.5938     
Epoch 3/10

 32/800 [>.............................] - ETA: 0s - loss: 1.5719 - acc: 0.7188
416/800 [==============>...............] - ETA: 0s - loss: 1.8082 - acc: 0.6635
800/800 [==============================] - 0s - loss: 4.1655 - acc: 0.5687     
Epoch 4/10

 32/800 [>.............................] - ETA: 0s - loss: 1.5698 - acc: 0.6875
384/800 [=============>................] - ETA: 0s - loss: 3.8528 - acc: 0.6484
768/800 [===========================>..] - ETA: 0s - loss: 6.5808 - acc: 0.5326
800/800 [==============================] - 0s - loss: 6.6416 - acc: 0.5300     
Epoch 5/10

 32/800 [>.............................] - ETA: 0s - loss: 8.8212 - acc: 0.4062
384/800 [=============>................] - ETA: 0s - loss: 3.7414 - acc: 0.6302
672/800 [========================>.....] - ETA: 0s - loss: 4.4989 - acc: 0.6324
800/800 [==============================] - 0s - loss: 4.5469 - acc: 0.6275     
Epoch 6/10

 32/800 [>.............................] - ETA: 0s - loss: 1.6976 - acc: 0.7812
544/800 [===================>..........] - ETA: 0s - loss: 3.3736 - acc: 0.5551
800/800 [==============================] - 0s - loss: 3.5173 - acc: 0.5725     
Epoch 7/10

 32/800 [>.............................] - ETA: 0s - loss: 2.6796 - acc: 0.6562
416/800 [==============>...............] - ETA: 0s - loss: 3.7487 - acc: 0.5625
768/800 [===========================>..] - ETA: 0s - loss: 3.2615 - acc: 0.5872
800/800 [==============================] - 0s - loss: 3.2155 - acc: 0.5900     
Epoch 8/10

 32/800 [>.............................] - ETA: 0s - loss: 4.0012 - acc: 0.5625
512/800 [==================>...........] - ETA: 0s - loss: 3.3170 - acc: 0.6426
800/800 [==============================] - 0s - loss: 3.4408 - acc: 0.6212     
Epoch 9/10

 32/800 [>.............................] - ETA: 0s - loss: 2.4099 - acc: 0.6250
352/800 [============>.................] - ETA: 0s - loss: 2.9775 - acc: 0.6136
800/800 [==============================] - 0s - loss: 3.5166 - acc: 0.6050     
Epoch 10/10

 32/800 [>.............................] - ETA: 0s - loss: 4.1211 - acc: 0.6250
480/800 [=================>............] - ETA: 0s - loss: 3.1673 - acc: 0.6271
800/800 [==============================] - 0s - loss: 3.4439 - acc: 0.6212     
[3.4499431e-06 4.6942978e-06 3.4443483e-01 4.7438064e-01 4.9725961e-05
 1.1824530e-05 2.2117827e-06 1.3059487e-03 5.7318925e-07 3.7774635e-06
 6.7048757e-05 3.5165675e-07 2.4876420e-06 1.0989519e-01 1.3865958e-05
 4.2536540e-05 2.1679568e-04 4.8920333e-06 1.9447519e-07 1.9939220e-02
 5.7432823e-09 4.5387522e-05 3.0782949e-06 4.6623463e-05 4.0190098e-01
 1.6899225e-05 1.6799664e-05 6.0698360e-01 1.6183445e-05 3.9149760e-11
 2.9503619e-03 4.4596517e-01 1.8982140e-05 1.9093385e-04 1.1415260e-03
 1.4003773e-06 3.8733878e-04 1.5822739e-05 5.8243058e-06 7.5586718e-06
 5.5977551e-04 1.9685838e-03 8.2695215e-06 4.3112327e-06 1.6129253e-03
 2.5958147e-07 1.2335786e-05 3.3474275e-06 1.4571147e-05 9.0591195e-05
 1.9068281e-01 3.9595510e-12 3.2720398e-02 4.5461073e-03 5.5078093e-08
 1.2205882e-03 2.2930658e-06 4.6269490e-09 9.9761601e-05 1.8982140e-05
 2.0339584e-07 2.9780850e-04 1.5179692e-08 1.4766613e-05 1.0989794e-04
 2.2191429e-07 4.1767027e-05 7.3901206e-06 1.4897196e-05 1.4258951e-01
 6.7018707e-05 8.9891943e-09 6.5085483e-06 4.2066084e-08 1.7098866e-05
 4.3585212e-03 5.3918891e-04 8.0162933e-04 1.5955264e-05 7.7992696e-10
 5.4394255e-05 5.8538194e-06 2.9566660e-04 3.0697098e-05 9.9382181e-05
 2.6495405e-07 3.9269122e-05 7.0899836e-04 7.9487418e-06 3.4395125e-05
 8.4112571e-06]

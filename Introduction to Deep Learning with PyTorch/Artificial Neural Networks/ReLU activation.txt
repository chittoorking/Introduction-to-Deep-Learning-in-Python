In this exercise, we have the same settings as the previous exercise. But now we are going to build a neural network which has non-linearity.
By doing so, we are going to convince ourselves that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer.



We have already instantiated the ReLU activation function called relu() for you.

Instructions
Apply the non-linearity to the two hidden layers and print the result.
Apply the non-linearity to the product of first two weights.
Multiply the result of the previous step with weight_3.
Multiply input_layer with weight and print the results.

# Instantiate non-linearity
relu = nn.ReLU()

# Apply non-linearity on the hidden layers
hidden_1_activated = relu(torch.matmul(input_layer, weight_1))
hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))
print(torch.matmul(hidden_2_activated, weight_3))

# Apply non-linearity in the product of first two weights. 
weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))

# Multiply `weight_composed_1_activated` with `weight_3
weight = torch.matmul(weight_composed_1_activated, weight_3)

# Multiply input_layer with weight
print(torch.matmul(input_layer, weight))


output:
tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])
tensor([[-0.2115, -0.4782,  4.0437,  3.0415]])

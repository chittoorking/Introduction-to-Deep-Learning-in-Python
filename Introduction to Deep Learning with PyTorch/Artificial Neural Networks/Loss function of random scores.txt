If the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch.
The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function.

Instructions
Import torch and torch.nn as nn
Initialize logits with a random tensor of shape (1, 1000) and ground_truth with a tensor containing the number 111.
Instantiate the cross-entropy loss in a variable called criterion.
Calculate and print the loss function.

# Import torch and torch.nn
import torch
import torch.nn as nn

# Initialize logits and ground truth
logits = torch.rand(1,1000)
ground_truth = torch.tensor([111])

# Instantiate cross-entropy loss
criterion=nn.CrossEntropyLoss()

# Calculate and print the loss
loss = criterion(logits,ground_truth)
print(loss)

output:

tensor(7.1930)


The score is close to -ln(1/1000) = 6.9.
This is not surprising, considering that scores were random and close to each other, so the probability for each class was approximately the same (1/1000) = 0.001.

Dropout is used to regularize fully-connected layers.
Batch-normalization is used to make the training of convolutional neural networks more efficient, while at the same time having regularization effects.
You are going to implement the __init__ method of a small convolutional neural network, with batch-normalization. 
The feature extraction part of the CNN will contain the following modules (in order):
convolution, max-pool, activation, batch-norm, convolution, max-pool, relu, batch-norm.

The first convolutional layer will contain 10 output channels, while the second will contain 20 output channels. 
As always, we are going to use MNIST dataset, with images having shape (28, 28) in grayscale format (1 channel). 
In all cases, the size of the filter should be 3, the stride should be 1 and the padding should be 1.

Instructions
Implement the feature extraction part of the network, using the description in the context.
Implement the fully-connected (classifier) part of the network.


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Implement the sequential module for feature extraction
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),
            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))
        
        # Implement the fully connected layer for classification
        self.fc = nn.Linear(in_features=7*7*20, out_features=10)
